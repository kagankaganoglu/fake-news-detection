{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PyTBG-xPqKTD"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, Embedding, LSTM, Bidirectional, Dense, Concatenate, Flatten\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Concatenate, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fWtrv8M1qTHA"
   },
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None,\n",
    "                     names=['label','file_name','label_text','statement','topic',\n",
    "                            'speaker','speaker_job','state','party','barely_true_counts',\n",
    "                            'false_counts','half_true_counts','mostly_true_counts',\n",
    "                            'pants_on_fire_counts','venue','extracted_context'])\n",
    "    return df\n",
    "\n",
    "train_df = load_data('dataset/train2.tsv')\n",
    "val_df = load_data('dataset/val2.tsv')\n",
    "test_df = load_data('dataset/test2.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O99uRA1gz978",
    "outputId": "df848c93-1cdc-4c81-c22e-813780ad575c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  2\n",
      "After:  0\n"
     ]
    }
   ],
   "source": [
    "# There are 2 null values for speaker in the training data. Here we drop them\n",
    "print(\"Before: \", train_df['speaker'].isnull().sum())\n",
    "train_df = train_df.dropna(subset=['speaker'])\n",
    "print(\"After: \", train_df['speaker'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "kg5372AA0YFr",
    "outputId": "94691066-0647-48a4-fbf2-4d19fa145e4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>says annies list political group supports thir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>decline coal start started natural gas took st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hillary clinton agrees john mccain voting give...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>health care reform legislation likely mandate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>economic turnaround started end term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10235</th>\n",
       "      <td>larger number shark attacks florida cases vote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10236</th>\n",
       "      <td>democrats become party atlanta metro area blacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10237</th>\n",
       "      <td>says alternative social security operates galv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10238</th>\n",
       "      <td>lifting cuban embargo allowing travel cuba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10239</th>\n",
       "      <td>department veterans affairs manual telling vet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10238 rows × 1 columns</p>\n",
       "</div><br><label><b>dtype:</b> object</label>"
      ],
      "text/plain": [
       "0        says annies list political group supports thir...\n",
       "1        decline coal start started natural gas took st...\n",
       "2        hillary clinton agrees john mccain voting give...\n",
       "3        health care reform legislation likely mandate ...\n",
       "4                     economic turnaround started end term\n",
       "                               ...                        \n",
       "10235    larger number shark attacks florida cases vote...\n",
       "10236     democrats become party atlanta metro area blacks\n",
       "10237    says alternative social security operates galv...\n",
       "10238           lifting cuban embargo allowing travel cuba\n",
       "10239    department veterans affairs manual telling vet...\n",
       "Name: statement, Length: 10238, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_data(data):\n",
    "\n",
    "    data['statement'] = data['statement'].fillna('')  # Fill empty values as string\n",
    "    data['statement'] = data['statement'].str.lower()  # Convert to lowercase\n",
    "    data['statement'] = data['statement'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))  # Remove punctuation\n",
    "    data['statement'] = data['statement'].apply(lambda x: re.sub(r'\\d+', '', x))  # Remove numbers\n",
    "\n",
    "    # len(word) > 2 might be unncessary. Let's try\n",
    "    data['statement'] = data['statement'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words and len(word) > 2]))  # Remove stopwords\n",
    "\n",
    "    data['processed_statement'] = data['statement'].apply(lambda x: word_tokenize(x))    # Tokenize\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()  # Initialize lemmatizer\n",
    "    data['processed_statement'] = data['processed_statement'].apply(\n",
    "        lambda token_list: [lemmatizer.lemmatize(word) for word in token_list])\n",
    "\n",
    "    return data\n",
    "\n",
    "train_df = preprocess_data(train_df)\n",
    "val_df = preprocess_data(val_df)\n",
    "test_df = preprocess_data(test_df)\n",
    "train_df[\"statement\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EjkZn5YQHirU"
   },
   "outputs": [],
   "source": [
    "# Convert processed_statement from list of words to strings\n",
    "train_texts = train_df['processed_statement'].apply(lambda x: ' '.join(x))\n",
    "val_texts = val_df['processed_statement'].apply(lambda x: ' '.join(x))\n",
    "test_texts = test_df['processed_statement'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_df['label_text'])\n",
    "val_labels = label_encoder.transform(val_df['label_text'])\n",
    "test_labels = label_encoder.transform(test_df['label_text'])\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "# Convert texts to sequences (i.e., list of integers corresponding to words)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "# Pad Sequences to Ensure Consistent Length\n",
    "max_len = 100\n",
    "train_sequences = pad_sequences(train_sequences, maxlen=max_len, padding='post')\n",
    "val_sequences = pad_sequences(val_sequences, maxlen=max_len, padding='post')\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# Load GloVe Embeddings\n",
    "def load_glove_embeddings(glove_path, vocab):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    # Create an embedding matrix\n",
    "    embedding_matrix = np.zeros((len(vocab) + 1, 100))\n",
    "    for word, i in vocab.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "# Get vocabulary from tokenizer\n",
    "vocab = tokenizer.word_index\n",
    "\n",
    "# Load GloVe embeddings (replace with the path to your downloaded GloVe file)\n",
    "embedding_matrix = load_glove_embeddings('glove.6B.100d.txt', vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "_av_BsAvIn35",
    "outputId": "3cc88ce7-02af-46d7-997d-104a5b8d65cb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ statement_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,073,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bilstm_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">34,048</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ statement_input (\u001b[38;5;33mInputLayer\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding_layer (\u001b[38;5;33mEmbedding\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │       \u001b[38;5;34m1,073,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bilstm_layer (\u001b[38;5;33mBidirectional\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m34,048\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ output_layer (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)                   │             \u001b[38;5;34m390\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,107,438</span> (4.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,107,438\u001b[0m (4.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,438</span> (134.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,438\u001b[0m (134.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,073,000</span> (4.09 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,073,000\u001b[0m (4.09 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replicating the model from the paper:\n",
    "# \"Where is your Evidence: Improving Fact-checking by Justification Modeling\"\n",
    "\n",
    "# Define the BiLSTM Model\n",
    "# Input Layer\n",
    "statement_input = Input(shape=(max_len,), name='statement_input')\n",
    "\n",
    "# Embedding Layer with GloVe embeddings\n",
    "embedding_layer = Embedding(\n",
    "    input_dim=len(vocab) + 1,  # Add 1 for padding\n",
    "    output_dim=100,            # GloVe embedding dimension\n",
    "    weights=[embedding_matrix],  # Set the pre-trained weights\n",
    "    trainable=False,            # Freeze GloVe embeddings\n",
    "    name='embedding_layer'\n",
    ")\n",
    "\n",
    "# Apply the embedding layer\n",
    "embedding = embedding_layer(statement_input)\n",
    "\n",
    "# BiLSTM Layer\n",
    "bilstm = Bidirectional(LSTM(32), name='bilstm_layer')(embedding)\n",
    "\n",
    "# Softmax Output Layer\n",
    "output = Dense(len(label_encoder.classes_), activation='softmax', name='output_layer')(bilstm)\n",
    "\n",
    "# Compile the Model\n",
    "model_s = Model(inputs=statement_input, outputs=output)\n",
    "model_s.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model_s.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5D2N323zF378",
    "outputId": "39d72351-dc74-466e-90d4-858d4d201a4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 77ms/step - accuracy: 0.2127 - loss: 1.7537 - val_accuracy: 0.2562 - val_loss: 1.7214\n",
      "Epoch 2/10\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 75ms/step - accuracy: 0.2546 - loss: 1.7101 - val_accuracy: 0.2586 - val_loss: 1.7064\n",
      "Epoch 3/10\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 72ms/step - accuracy: 0.2753 - loss: 1.6881 - val_accuracy: 0.2500 - val_loss: 1.7076\n",
      "Epoch 4/10\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 78ms/step - accuracy: 0.2858 - loss: 1.6666 - val_accuracy: 0.2445 - val_loss: 1.7082\n",
      "Epoch 5/10\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 70ms/step - accuracy: 0.3032 - loss: 1.6471 - val_accuracy: 0.2445 - val_loss: 1.7238\n",
      "Epoch 6/10\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 71ms/step - accuracy: 0.3260 - loss: 1.6288 - val_accuracy: 0.2375 - val_loss: 1.7427\n",
      "Epoch 7/10\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 70ms/step - accuracy: 0.3333 - loss: 1.5974 - val_accuracy: 0.2391 - val_loss: 1.7311\n",
      "Epoch 8/10\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 72ms/step - accuracy: 0.3502 - loss: 1.5795 - val_accuracy: 0.2399 - val_loss: 1.7455\n",
      "Epoch 9/10\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 79ms/step - accuracy: 0.3661 - loss: 1.5527 - val_accuracy: 0.2204 - val_loss: 1.7567\n",
      "Epoch 10/10\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 73ms/step - accuracy: 0.3920 - loss: 1.5137 - val_accuracy: 0.2165 - val_loss: 1.7603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7a8a9c8e2b60>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# One-hot encode the labels\n",
    "train_labels_one_hot = to_categorical(train_labels, num_classes=len(label_encoder.classes_))\n",
    "val_labels_one_hot = to_categorical(val_labels, num_classes=len(label_encoder.classes_))\n",
    "test_labels_one_hot = to_categorical(test_labels, num_classes=len(label_encoder.classes_))\n",
    "\n",
    "# Train the Model\n",
    "model_s.fit(train_sequences, train_labels_one_hot, validation_data=(val_sequences, val_labels_one_hot), epochs=10, batch_size=32) # We train for 10 epochs like in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jABBIqs-ItKu",
    "outputId": "8588a0d9-ce5d-4621-ee88-d8ba0f736558"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.1976 - loss: 1.7984\n",
      "Test Loss: 1.777524471282959\n",
      "Test Accuracy: 0.21704813838005066\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on Test Data\n",
    "test_loss, test_accuracy = model_s.evaluate(test_sequences, test_labels_one_hot)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\") # %24.6\n",
    "# The paper performs with %23 accuracy on the test set while our model performs with %24.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UGH7ZBc6Lny3",
    "outputId": "f698ad54-dc2e-4fc4-a567-52032a2c32b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.21488571986716157\n",
      "Validation Accuracy: 0.1822429906542056\n",
      "Test Accuracy: 0.20599842146803474\n"
     ]
    }
   ],
   "source": [
    "# Train a logistic regression model\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=2000, multi_class='ovr')\n",
    "lr_model.fit(train_sequences, train_labels)\n",
    "\n",
    "# Make predictions\n",
    "train_preds = lr_model.predict(train_sequences)\n",
    "val_preds = lr_model.predict(val_sequences)\n",
    "test_preds = lr_model.predict(test_sequences)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'Training Accuracy: {accuracy_score(train_labels, train_preds)}') # %21.5\n",
    "print(f'Validation Accuracy: {accuracy_score(val_labels, val_preds)}') # %18.2\n",
    "print(f'Test Accuracy: {accuracy_score(test_labels, test_preds)}')  # %20.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XYe7jSuZQYw-",
    "outputId": "dd6ca63d-5342-4548-fb49-0f837572a36e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy (SVM): 0.21058800546981832\n",
      "Validation Accuracy (SVM): 0.19937694704049844\n",
      "Test Accuracy (SVM): 0.19889502762430938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Train the SVM model\n",
    "svm_model = LinearSVC(random_state=42)\n",
    "svm_model.fit(train_sequences, train_labels)\n",
    "\n",
    "# Make predictions\n",
    "train_preds_svm = svm_model.predict(train_sequences)\n",
    "val_preds_svm = svm_model.predict(val_sequences)\n",
    "test_preds_svm = svm_model.predict(test_sequences)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'Training Accuracy (SVM): {accuracy_score(train_labels, train_preds_svm)}') # %21\n",
    "print(f'Validation Accuracy (SVM): {accuracy_score(val_labels, val_preds_svm)}') # %19.9\n",
    "print(f'Test Accuracy (SVM): {accuracy_score(test_labels, test_preds_svm)}') # 19.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-o9Ci99YRU2Y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
