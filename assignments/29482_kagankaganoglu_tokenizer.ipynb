{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **My Tokenizer**\n",
        "\n",
        "In this assignment, you are asked to create your own word tokenizer without the help of external tokenizers. Steps to the assignment:\n",
        "1. Choose one of the corpora from nltk.corpus list given - assign it to corpus_name\n",
        "1. Create your tokenizer in the code block - tokenize the selected corpus into token_list\n",
        "1. Give the raw corpus text, corpus_raw, and the my_token_list to the evaluation block\n",
        "\n",
        "Only splitting on whitespace is not enough. At least try two other improvements on the tokenization. Please write sufficient comments to show your reasoning.\n",
        "\n",
        "## Rules\n",
        "### Allowed:\n",
        " - Choosing a top-down tokenizer or bottom-up tokenizer\n",
        " - Using regular expressions library (import re)\n",
        " - Adding additional coding blocks\n",
        " - Having an additional dataset if you are creating a bottom-up tokenizer but you need to be able to run the code standalone.\n",
        "\n",
        "### Not allowed:\n",
        " - Using tokenizer libraries such as nltk.tokenize, or any other external libraries to tokenize.\n",
        " - Changing the contents of the evaluation block at the end of the notebook.\n",
        "\n",
        "## Assignment Report\n",
        "Please write a short assignment report at the end of the notebook (max 500 words). Please include all of the following points in the report:\n",
        " - Corpus name and the selection reason\n",
        " - Design of the tokenizer and reasoning\n",
        " - Challenges you have faced while writing the tokenizer and challenges with the specific corpus\n",
        " - Limitations of your approach\n",
        " - Possible improvements to the system\n",
        "\n",
        "## Grading\n",
        "You will be graded with the following criteria:\n",
        " - running complete code (0.5),\n",
        " - tokenizer algorithm (2),\n",
        " - clear commenting (0.5),\n",
        " - evaluation score - comparison with nltk word tokenizer (at most 1 point),\n",
        " - assignment report (1).\n",
        "\n",
        "## Submission\n",
        "\n",
        "Submission will be made to SUCourse. Please submit your file using the following naming convention.\n",
        "\n",
        "\n",
        "`studentid_studentname_tokenizer.ipynb Â - ex. 26744_aysegulrana_tokenizer.ipynb`\n",
        "\n",
        "\n",
        "**Deadline is October 22nd, 5pm.**"
      ],
      "metadata": {
        "id": "1TXP5nalr9ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_tokenizer(corpus_raw):\n",
        "    '''\n",
        "    type corpus_raw: string\n",
        "    param corpus_raw: The raw output of the corpus to be tokenized\n",
        "    rtype: list\n",
        "    return: a list of tokens extracted from the corpus_raw\n",
        "    '''\n",
        "\n",
        "    # write your tokenizer here and apply to corpus_raw. Return the resulting token_list.\n",
        "    # you are NOT allowed to use external tokenizers such as word_tokenize from nltk.\n",
        "    # Only splitting on whitespace is not enough. At least try two other improvements on the tokenization.\n",
        "\n",
        "    # Lower the letters\n",
        "    first_tokens = corpus_raw.lower()\n",
        "\n",
        "    # Tokenize words. Split by apostrophes and rest of the word, for example, tokyo's is tokenized as \"tokyo\" and \"'s\". Also, tokenize ; > & ( ) \" characters alone.\n",
        "    first_tokens = re.findall(r\"'(?:\\w+)?|;|>|&|\\(|\\)|\\\"|[^\\s';>&\\(\\)\\\"]+\", first_tokens)\n",
        "\n",
        "    # New list to hold the final tokens\n",
        "    token_list = []\n",
        "\n",
        "    # This loop splits any period and comma character but only if it has whitespace after it (so numbers like 1.76 2,000,132 or abbrevations like u.s. isn't splitted)\n",
        "    for token in first_tokens:\n",
        "        # Check if the token has a period at the end and there are no period in it (so we don't split dots at the end of abbrevations like u.s.)\n",
        "        if (token.endswith('.') and ('.' not in token[:-1])):\n",
        "            token_list.append(token[:-1])  # Append the part before the dot\n",
        "            token_list.append('.')          # Append the dot\n",
        "        # Check if token has comma at the end\n",
        "        elif token.endswith(','):\n",
        "            token_list.append(token[:-1])  # Append the part before the comma\n",
        "            token_list.append(',')          # Append the comma\n",
        "        else:\n",
        "            token_list.append(token)\n",
        "\n",
        "    return token_list"
      ],
      "metadata": {
        "id": "-jYyH_qz3Lii"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are allowed to add code blocks above to use for your tokenizer or evaluate it.\n",
        "\n"
      ],
      "metadata": {
        "id": "5HP0awlu3jci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#main code to run your tokenizer.\n",
        "\n",
        "#import your libraries here\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "#select the corpus name from the list below\n",
        "#gutenberg, webtext, reuters, product_reviews_2\n",
        "\n",
        "corpus_name = 'reuters'\n",
        "\n",
        "#download the corpus and import it.\n",
        "nltk.download(corpus_name)\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "\n",
        "#get the raw text output of the corpus to the corpus_raw variable.\n",
        "corpus_raw = reuters.raw()\n",
        "\n",
        "#call your tokenizer method\n",
        "my_tokenized_list = my_tokenizer(corpus_raw)\n",
        "\n"
      ],
      "metadata": {
        "id": "-ZF4WJjKxZfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1fb6d7a-f856-45d4-8b0f-12b0f5c86ca2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Please do not touch the code below that will evaluate your tokenizer with the nltk word tokenizer. You will get zero points from evaluation if you do so."
      ],
      "metadata": {
        "id": "iuTDStwY36cT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_score(set_a, set_b):\n",
        "    '''\n",
        "    type set_a: set\n",
        "    param set_a: The first set to be compared\n",
        "    type set_b: set\n",
        "    param set_b: The tokens extracted from the corpus_raw\n",
        "    rtype: float\n",
        "    return: similarity score with two sets using Jaccard similarity.\n",
        "    '''\n",
        "\n",
        "    jaccard_similarity = float(len(set_a.intersection(set_b)) / len(set_a.union(set_b)))\n",
        "\n",
        "    return jaccard_similarity"
      ],
      "metadata": {
        "id": "8txzw8Ag8ysD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk import punkt\n",
        "\n",
        "def evaluation(corpus_raw, token_list):\n",
        "    '''\n",
        "    type corpus_raw: string\n",
        "    param corpus_raw: The raw output of the corpus\n",
        "    type token_list: list\n",
        "    param token_list: The tokens extracted from the corpus_raw\n",
        "    rtype: float\n",
        "    return: comparison score with the given token list and the nltk tokenizer.\n",
        "    '''\n",
        "\n",
        "    #The comparison score only looks at the tokens but not the frequencies of the tokens.\n",
        "    #we assume case folding is already applied to the token_list\n",
        "    corpus_raw = corpus_raw.lower()\n",
        "    nltk_tokens = word_tokenize(corpus_raw, language='english')\n",
        "\n",
        "    score = similarity_score(set(token_list), set(nltk_tokens))\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "6wUTqReb36Hg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "632867f2-ac0b-4686-e2f8-c0e9e2b2c507"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation\n",
        "\n",
        "eval_score= evaluation(corpus_raw, my_tokenized_list)\n",
        "\n",
        "print('The similarity score is {:.2f}'.format(eval_score))"
      ],
      "metadata": {
        "id": "6Vt_uSBF-NHm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d4c8004-95d6-4373-f203-73649664dedc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The similarity score is 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please write your report below using clear headlines with markdown syntax."
      ],
      "metadata": {
        "id": "keZ3UBqh4hgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment Report**\n",
        "\n",
        "## Corpus Name and Selection Reasoning\n",
        "\n",
        "For this assignment, I have chosen the Reuters corpus from the NLTK library. It is a well-known corpus consisting of a large collection of news documents, covering diverse topics such as economics, international events, and various industries. This variety makes it suitable for testing my tokenizer on abbreviations, special names, complex punctuation, contractions, possessives, and diverse sentence structures. Overall, it provides a comprehensive challenge for evaluating the robustness of the tokenizer.\n",
        "\n",
        "## Design of the Tokenizer and Reasoning\n",
        "\n",
        "The tokenizer is designed to handle common tokenization tasks while addressing specific language challenges. Initially, I converted all words to lowercase and split the text by whitespace and apostrophes, as contractions and possessives like \"it's\" and \"John's\" are common in English. The part following the apostrophe is kept together with the apostrophe to retain its meaning.\n",
        "\n",
        "Additionally, I ensured that punctuation marks such as ;, >, &, (, ), and quotation marks (\") are tokenized independently, as these are typically standalone symbols. I avoided splitting by all punctuation marks, as that would cause issues with abbreviations like \"U.S.\" and numbers like \"2,000.\" To address this, I implemented a rule that only splits periods or commas if they appear at the end of a word and are followed by whitespace, without other periods or commas in the word. This preserves correct tokenization for cases like numbers and abbreviations.\n",
        "\n",
        "## Challenges Faced\n",
        "\n",
        "The main challenge I encountered was determining how to handle punctuation in the corpus, as the Reuters corpus included many punctuations in different context. Simply tokenizing each punctuation mark separately didn't work well, as it caused issues with abbreviations, the use of dashes between words, and numbers with decimals, which lowered the similarity score. To address this, I explored different approaches, such as selectively tokenizing only certain punctuation marks, keeping periods and commas with the preceding word, and separating the part after the apostrophe from the apostrophe itself. Ultimately, the final solution I found balanced these cases and efficiently handled punctuations.\n",
        "\n",
        "## Limitations of the Approach\n",
        "\n",
        "One limitation of the tokenizer is its handling of special words. Since all words are converted to lowercase, it might not be able to differentiate between proper nouns or specialized terms and regular words. Additionally, the tokenizer doesn't account for complex cases involving multiple punctuation marks, such as email addresses or URLs. Another limitation is that the tokenizer is designed with the assumption of English-language text and may not generalize well to other languages or scripts.\n",
        "\n",
        "## Possible Improvements\n",
        "\n",
        "To improve the tokenizer, more sophisticated regular expressions that recognize patterns such as email addresses, URLs, and dates could be implemented. Another potential improvement would be to incorporate handling for multi-word expressions and named entities (e.g., \"New York City\" as one token)."
      ],
      "metadata": {
        "id": "9juDLO2cA6o5"
      }
    }
  ]
}